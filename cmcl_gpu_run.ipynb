{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# CMCL Recommender System - GPU Run (Goodreads Poetry)\n",
                "\n",
                "This notebook is designed to train the Cross-Modal Contrastive Learning (CMCL) model on the Goodreads Poetry dataset using a GPU. It is optimized for Google Colab **FREE TIER** (12.7 GB RAM limit)."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup Codebase\n",
                "Cloning the repository from GitHub. If it already exists, we pull the latest changes."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "if os.path.exists(\"PGMS_for_Recommender_Systems\"):\n",
                "    %cd PGMS_for_Recommender_Systems\n",
                "    !git pull\n",
                "else:\n",
                "    !git clone https://github.com/mohdfaour03/PGMS_for_Recommender_Systems.git\n",
                "    %cd PGMS_for_Recommender_Systems\n",
                "!pip install -e ."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Setup and Dependencies\n",
                "Install necessary packages."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install torch pandas numpy scipy scikit-learn tqdm pyyaml requests"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Imports and Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "from pathlib import Path\n",
                "import json\n",
                "import torch\n",
                "import gc\n",
                "\n",
                "# Ensure the current directory is in the path so we can import coldstart\n",
                "sys.path.append(\".\")\n",
                "\n",
                "try:\n",
                "    from coldstart.src import pipeline\n",
                "    from coldstart.src.notebook_utils import build_goodreads_interaction_frame, _read_simple_yaml\n",
                "except ImportError:\n",
                "    print(\"❌ ERROR: Could not import 'coldstart'. Please ensure the repository was cloned successfully.\")\n",
                "    raise\n",
                "\n",
                "# Check for GPU\n",
                "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"Device: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
                "else:\n",
                "    print(\"⚠️ WARNING: GPU not detected. Go to Runtime > Change runtime type > T4 GPU to enable it.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Data Loading\n",
                "Load the Goodreads Poetry dataset. **Using 100k interactions to fit within Colab free tier RAM limit (12.7 GB).**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "DATA_DIR = Path(\"coldstart/data\")\n",
                "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
                "GOODREADS_PATH = DATA_DIR / \"goodreads_poetry_100k.csv\"\n",
                "RUN_DIR = Path(\"coldstart/output/goodreads_gpu_run\")\n",
                "RUN_DIR.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "# CRITICAL: Using 100k interactions instead of 300k to avoid RAM crash\n",
                "INTERACTION_LIMIT = 100000\n",
                "\n",
                "if not GOODREADS_PATH.exists():\n",
                "    print(f\"Downloading/Loading Goodreads Poetry dataset ({INTERACTION_LIMIT} interactions)...\")\n",
                "    df = build_goodreads_interaction_frame(genre=\"poetry\", limit=INTERACTION_LIMIT)\n",
                "    df.to_csv(GOODREADS_PATH, index=False)\n",
                "    print(f\"Saved Goodreads Poetry dataset to {GOODREADS_PATH}\")\n",
                "else:\n",
                "    print(f\"Goodreads Poetry dataset already exists at {GOODREADS_PATH}\")\n",
                "\n",
                "gc.collect()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Pipeline Execution\n",
                "Run the training pipeline. We enable `prefer_gpu=True` and use conservative batch sizes for RAM safety."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load base config\n",
                "config = _read_simple_yaml(\"coldstart/configs/base.yaml\")\n",
                "\n",
                "# Prepare Dataset\n",
                "print(\"Preparing dataset...\")\n",
                "pipeline.prepare_dataset(\n",
                "    GOODREADS_PATH,\n",
                "    RUN_DIR,\n",
                "    tfidf_params=config.get(\"tfidf\", {}),\n",
                "    cold_item_frac=0.2,\n",
                "    seed=42,\n",
                "    interaction_limit=INTERACTION_LIMIT,\n",
                ")\n",
                "\n",
                "gc.collect()\n",
                "\n",
                "# Train and Evaluate\n",
                "print(\"Starting training on GPU...\")\n",
                "metrics = pipeline.train_and_evaluate_content_model(\n",
                "    RUN_DIR,\n",
                "    k_factors=16,\n",
                "    k_eval=5,\n",
                "    mf_reg=float(config.get(\"mf\", {}).get(\"reg\", 0.02)),\n",
                "    mf_iters=20, # Reduced from 30 for speed\n",
                "    mf_lr=float(config.get(\"mf\", {}).get(\"lr\", 0.02)),\n",
                "    seed=42,\n",
                "    ctrlite_reg=float(config.get(\"ctrlite\", {}).get(\"reg\", 0.01)),\n",
                "    ctrlite_lr=float(config.get(\"ctrlite\", {}).get(\"lr\", 0.1)),\n",
                "    ctrlite_iters=50, # Reduced from 80 for speed\n",
                "    adaptive=False,\n",
                "    model=\"ctrlite,cdl,cmcl\", # Run all for comparison\n",
                "    a2f_cfg=config.get(\"a2f\", {}),\n",
                "    ctpf_cfg=config.get(\"ctpf\", {}),\n",
                "    cdl_cfg=config.get(\"cdl\", {}),\n",
                "    hft_cfg=config.get(\"hft\", {}),\n",
                "    micm_cfg=config.get(\"micm\", {}),\n",
                "    cmcl_cfg={\"iters\": 10},\n",
                "    backend=\"torch\",\n",
                "    prefer_gpu=True, # ENABLE GPU\n",
                "    mf_cfg={\n",
                "        \"batch_size\": 1024,\n",
                "        \"score_batch_size\": 2048,\n",
                "        \"infer_batch_size\": 2048,\n",
                "        \"ctrlite_batch_size\": 1024,\n",
                "    },\n",
                ")\n",
                "\n",
                "gc.collect()\n",
                "print(\"\\n✅ Training completed successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Results"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n=== Final Results ===\")\n",
                "print(json.dumps(metrics, indent=2))\n",
                "with open(RUN_DIR / \"metrics.json\", \"w\") as f:\n",
                "    json.dump(metrics, f, indent=2)\n",
                "\n",
                "print(f\"\\nResults saved to {RUN_DIR / 'metrics.json'}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}